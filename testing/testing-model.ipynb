{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json,os,sys,logging,pprint,re,string,courseraLib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample_rate=1 #1/10000\n",
    "testing_set=\\\n",
    "open('/home/yewenhe0904/Developing/ds-capstone-project/testing-data/mixed-testing.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initial call to print 0% progress\n",
    "i = 0\n",
    "l = len(list(testingSet))\n",
    "courseraLib.printProgressBar(i, l, prefix = 'Sampling '+f+\":\", suffix = 'Complete')\n",
    "for line in fileObj:\n",
    "    \n",
    "    i+=1\n",
    "    courseraLib.printProgressBar(i, l, prefix = 'Sampling '+f+\":\", suffix = 'Complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nGramDict=json.load(open('/home/yewenhe0904/Developing/ds-capstone-project/ignoredFiles/Sample-Dict/mixed-nGramDict_skim_1.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cleanInputPredict(test_input):\n",
    "    cleanedInput=[]\n",
    "    test_tokens=test_input.strip().lower().split()\n",
    "    test_input=re.sub('\\n+',\" \",test_input)\n",
    "    for item in test_tokens:\n",
    "        item = item.strip(string.punctuation)\n",
    "        item = re.sub(r\"^.*\\d+.*$\",\"<Quantity>\",item)\n",
    "        cleanedInput.append(item)\n",
    "    return cleanedInput\n",
    "\n",
    "def predictCore(input_tokens,output_list,out_len=5):\n",
    "    print (input_tokens)\n",
    "    if len(input_tokens)<1:\n",
    "        output_list+=(nGramDict['_r'])\n",
    "    else:\n",
    "        # probe if in ngram record\n",
    "        nGram_flag=True\n",
    "        i=0\n",
    "        p_Dict=nGramDict['_n']\n",
    "        while nGram_flag:\n",
    "            print(i)\n",
    "            if i==len(input_tokens):\n",
    "                break\n",
    "            if (input_tokens[i] in p_Dict):\n",
    "                r_wids=p_Dict[input_tokens[i]]['_r']\n",
    "                if ('_n' in p_Dict[input_tokens[i]]):\n",
    "                    p_Dict=p_Dict[input_tokens[i]]['_n']\n",
    "                    i+=1\n",
    "                else:\n",
    "                    if i<len(input_tokens)-1:\n",
    "                        nGram_flag=False\n",
    "                    break\n",
    "            else:\n",
    "                nGram_flag=False\n",
    "                break\n",
    "        # if true\n",
    "        if nGram_flag:\n",
    "            output_list+=r_wids\n",
    "            if len(output_list)<out_len:\n",
    "                del input_tokens[0]\n",
    "                predictCore(input_tokens,output_list,out_len=out_len)\n",
    "        # else false\n",
    "        else:\n",
    "            del input_tokens[0]\n",
    "            predictCore(input_tokens,output_list,out_len=out_len)\n",
    "\n",
    "def predictNgram(test_input,out_len):\n",
    "    #clean input string\n",
    "    input_tokens=cleanInputPredict(test_input)\n",
    "    if len(input_tokens)>nGramDict['_model']:\n",
    "        input_tokens=input_tokens[-nGramDict['_model']:]\n",
    "    output_list=[]\n",
    "    output_wlist=[]\n",
    "    output=[]\n",
    "    input_idtokens=[]\n",
    "    for w in input_tokens:\n",
    "        w.strip()\n",
    "        if w in nGramDict['_word2id']['_word']:\n",
    "            input_idtokens.append(nGramDict['_word2id']['_word'][w])\n",
    "        else:\n",
    "            input_idtokens.append('NA')\n",
    "    predictCore(input_idtokens,output_list,out_len)\n",
    "    for wid in output_list:\n",
    "        output_wlist.append(nGramDict['_id2word']['_id'][wid])\n",
    "    eosflag=True\n",
    "#     print(output_wlist)\n",
    "    for word in output_wlist:\n",
    "        if word=='<EOS>':\n",
    "            if eosflag:\n",
    "                output.append(',')\n",
    "                output.append('.')\n",
    "            eosflag=False\n",
    "        elif word==\"<Quantity>\":\n",
    "            continue\n",
    "        else:\n",
    "            output.append(word)\n",
    "        #dedupe\n",
    "        output_dp=[output[0]]\n",
    "        if len(output)>1:\n",
    "            for i in range(1,len(output)):\n",
    "                if output[i] not in output[:i]:\n",
    "                    output_dp.append(output[i])\n",
    "        if len(output_dp)>out_len:\n",
    "            output_dp=output_dp[0:out_len]\n",
    "    return output_dp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "predictNgram(test_input,8)     \n",
    "      \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
